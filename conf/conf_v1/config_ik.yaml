defaults:
  - _self_

data:
  data_source: ./data/
  indices:
    start: 0
    end: 100
  val_indices:
    start: 101
    end: 110
  num_instances: 100
  subsample: 16384
  batch_size: 64
  val_batch_size: 64
  max_num_links: 5
  load_ram: false
  shuffle: true
  num_workers: 8
  drop_last: true

models:
  joint_encoder:
    input_dim: 9
    output_dim: 128
    hidden_dims: [128, 128]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  link_encoder:
    input_dim: 1024
    output_dim: 128
    hidden_dims: [256, 256]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  transformer:
    input_dim: 128
    model_dim: 128
    output_dim: 128
    num_heads: 4
    num_layers: 4
    mlp_ratio: 4.0
    dropout: 0.1
    attn_dropout: 0.0
    activation: gelu
    use_positional_encoding: true
    max_length: 32
    norm_first: true
    final_layer_norm: true
  joint_value_decoder:
    input_dim: 128
    hidden_dims: [256, 128]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false

training:
  device: cuda
  dtype: float32
  lr: 1e-4
  num_epochs: 1e8
  wandb:
    enabled: true
    project: action_repr
    run_name: ik-transformer_v1_tanh
    save_interval: 50
    save_dir: ./checkpoints
  lr_schedules:
    - Type: Step
      Initial: 0.0005
      Interval: 500
      Factor: 0.5
  pretrained_ckpt: ./checkpoints/sdf-transformer_2/epoch_1500.pth
  test_ckpt: ./checkpoints/ik-transformer_v1/epoch_400.pth
  freeze_pretrained: true
