defaults:
  - _self_

data:
  data_source: ./data/
  indices:
    start: 0
    end: 100
  val_indices:
    start: 100
    end: 110
  num_instances: 100
  subsample: 16384
  batch_size: 32
  val_batch_size: 32
  max_num_links: 5
  load_ram: false
  shuffle: true
  num_workers: 0
  drop_last: true

models:
  joint_encoder:
    input_dim: 9
    output_dim: 128
    hidden_dims: [128, 128]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  link_encoder:
    input_dim: 257
    output_dim: 128
    hidden_dims: [256, 256]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  joint_value_encoder:
    embed_dim: 128
    use_positional: true
    mlp_hidden: [128]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  transformer:
    input_dim: 128
    model_dim: 128
    output_dim: 128
    num_heads: 4
    num_layers: 4
    mlp_ratio: 4.0
    dropout: 0.1
    attn_dropout: 0.0
    activation: gelu
    use_positional_encoding: true
    max_length: 32
    norm_first: true
    final_layer_norm: true
  decoder:
    latent_size: 128
    dims: [512, 512, 512, 512, 512, 512, 512, 512]
    dropout: [0, 1, 2, 3, 4, 5, 6, 7]
    dropout_prob: 0.2
    norm_layers: [0, 1, 2, 3, 4, 5, 6, 7]
    latent_in: [4]
    xyz_in_all: false
    use_tanh: false
    latent_dropout: false
    weight_norm: true

training:
  device: cuda
  dtype: float32
  debug_batch: true
  lr: 1e-4
  num_epochs: 1
  wandb:
    enabled: true
    project: action_repr
    run_name: sdf-transformer
