defaults:
  - _self_

data:
  data_source: ./data/
  indices:
    start: 0
    end: 100
  val_indices:
    start: 101
    end: 110
  num_instances: 100
  subsample: 16384
  batch_size: 64
  val_batch_size: 64
  max_num_links: 5
  load_ram: false
  shuffle: true
  num_workers: 8
  drop_last: true

models:
  joint_encoder:
    input_dim: 9
    output_dim: 128
    hidden_dims: [128, 128]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  link_encoder:
    input_dim: 1024
    output_dim: 128
    hidden_dims: [256, 256]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  joint_value_encoder:
    embed_dim: 128
    use_positional: true
    mlp_hidden: [128, 128]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false
  transformer:
    input_dim: 128
    model_dim: 128
    output_dim: 128
    num_heads: 4
    num_layers: 4
    mlp_ratio: 4.0
    dropout: 0.1
    attn_dropout: 0.0
    activation: gelu
    use_positional_encoding: true
    max_length: 32
    norm_first: true
    final_layer_norm: true
  joint_value_decoder:
    input_dim: 128
    hidden_dims: [256, 128]
    activation: gelu
    dropout: 0.0
    use_layer_norm: false

flow_matching:
  noise_std: 1.0
  time_embed_dim: 128
  time_mlp_hidden: 256

training:
  device: cuda
  dtype: float32
  lr: 1e-4
  num_epochs: 1e6
  validation_interval: 1000
  inference_steps: 10
  wandb:
    enabled: true
    project: action_repr
    run_name: ik-transformer_fm
    save_interval: 50
    save_dir: ./checkpoints
  lr_schedules: []
  pretrained_ckpt: null
  freeze_pretrained: false
